{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Super Resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anatol Kaczmarek 156038\\\n",
    "Dawid Siera 156044\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The problem we choose for this project was Super Resolution, but not the usual image super resolution, but video super resolution. The goal is to take a low resolution video, say 640x360 and create a higher resolution video, say 1280x720p from it with accuracy and image quality as high as possible and hopefully beating the common interpolation method. What the video part brings into the table is the fact that although video is just a sequence of images, consecutive frames usually share some similarities, and by looking at the previous and next frames we can try to predict the current frame with higher accuracy. The downside is increased computational time since we need to process multiple input images per one output frame, limitations in computational resources was the main problem we encountered. Since our solution makes use of information happening over time we will sometimes refer to it as temporal super resolution (TSR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We used the Inter4k video dataset, it consists of 1000 videos recorded in 4k at 60 fps. The videos are short, around 5s which amount to 300 frames per video and 300,000 frames in whole dataset. dataset of such size was absolutely infeasible for us to process in a reasonable time, so we decided to only use a part of it. Moreover to save computational resources we resized the videos to smaller resolutions, we experimented a bit with different resolutions before settling on 360p to 720p upscaling. Although our model is Fully Convolutional Neural Network which makes is agnostic on the image size, we assumed that by focusing on specific resolution we would get the best results, that's why all the results from now on refer to 360p->720p upscaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check an one of the videos in both resolutions below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"resources/test360.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('resources/test360.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"resources/test720.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('resources/test720.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We use a simple 6 layer fully convolutional model, based on convolutional layers and ReLU functions, but the first crucial step is to upscale the input image to the desired resolution. Since we do not use convolutional layers to increase the size of an input image we rely on regular bicubic interpolation to bring image to the desired size the we use our stack of trainable layers to reconstruct high quality image from low quality interpolation upscaling result. This also makes bicubic interpolation an ideal baseline for comparisons. The convolutional layers do not get padding because we didn't want the padded zeroes to interfere with image reconstructing effort, this mean that the output image is actually couple of pixels smaller than expected although this could be remedied by upsampling the image to slightly higher base size. Our model can be initialized with number of past/future frames to use in upscaling and upscale_factor (ratio of output and input resolutions), the concatenation happens in forward function where we stack the input frames along the channel dimension. We tried working with models using 1 and 2 future/past frames but we didn't get result high enough to justify higher computational demand so we sticked to 1 past/future frame. We also created a bigger variant of the architecture with bigger kernels and more neurons for comparison, unfortunately its increased training time forced us to decrease the number of epochs to the detriment of final results and the result was clearly underfitted model. Altough we can imagine someone with more computational resources making good use of the this bigger model we abandoned it in favor of smaller one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram of our model's architecture (the bigger variant):\n",
    "\n",
    "![](resources/Diagram.png)\n",
    "\n",
    "Below is the code for our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "class TSRCNN_small(nn.Module):\n",
    "    def __init__(self, frames_backward=2, frames_forward=2, upscale_factor=1.5):\n",
    "        super(TSRCNN_small, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(3 * (frames_backward + 1 + frames_forward), 64, kernel_size=9, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=5, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, back_frames: Tensor, low_res_frame: Tensor, forward_frames: Tensor) -> Tensor:\n",
    "        x = torch.cat([back_frames, low_res_frame, forward_frames], dim=1)\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TSRCNN_large(nn.Module):\n",
    "    def __init__(self, frames_backward=2, frames_forward=2, upscale_factor=1.5):\n",
    "        super(TSRCNN_large, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(3 * (frames_backward + 1 + frames_forward), 128, kernel_size=11, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=7, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, back_frames: Tensor, low_res_frame: Tensor, forward_frames: Tensor) -> Tensor:\n",
    "        x = torch.cat([back_frames, low_res_frame, forward_frames], dim=1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per torchtools, our smaller model has 51,203 parameters all of which are trainable, and they take 0.2 MB of memory. The bigger model has 222,723 trainable parameter which take 0.85 MB of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------\n",
      "                            Layer (type)    Output shape     Param shape      Param #     FLOPs basic           FLOPs\n",
      "=====================================================================================================================\n",
      "                                 Input *     1x3x360x640\n",
      "                                 Input *     1x3x360x640\n",
      "                                 Input *     1x3x360x640\n",
      "                   layers.0 (Upsample) *    1x9x720x1280                            0               0               0\n",
      "                     layers.1 (Conv2d) *   1x64x712x1272     64x9x9x9+64       46,720  42,254,659,584  42,312,622,080\n",
      "                       layers.2 (ReLU) *   1x64x712x1272                            0               0               0\n",
      "                     layers.3 (Conv2d) *   1x32x712x1272    32x64x1x1+32        2,080   1,854,799,872   1,883,781,120\n",
      "                       layers.4 (ReLU) *   1x32x712x1272                            0               0               0\n",
      "                     layers.5 (Conv2d) *    1x3x708x1268      3x32x5x5+3        2,403   2,154,585,600   2,157,278,832\n",
      "                   layers (Sequential)      1x3x708x1268                            0               0               0\n",
      "                        (TSRCNN_small)      1x3x708x1268                            0               0               0\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Total params: 51,203 (0.19532394409179688 MB)\n",
      "Total params (with aux): 51,203 (0.19532394409179688 MB)\n",
      "    Trainable params: 51,203 (0.19532394409179688 MB)\n",
      "    Non-trainable params: 0 (0.0 MB)\n",
      "Total flops (basic): 46,264,045,056 (46.264045056 billion)\n",
      "Total flops: 46,353,682,032 (46.353682032 billion)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "NOTE:\n",
      "    *: leaf modules\n",
      "    Flops is measured in multiply-adds. Multiply, divide, exp are treated the same for calculation, add is ignored except for bias.\n",
      "    Flops (basic) only calculates for convolution and linear layers (not inlcude bias)\n",
      "    Flops additionally calculates for bias, normalization (BatchNorm, LayerNorm, GroupNorm), RNN (RNN, LSTM, GRU) and attention layers\n",
      "        - activations (e.g. ReLU), operations implemented as functionals (e.g. add in a residual architecture) are not \n",
      "          calculated as they are usually neglectable.\n",
      "        - complex custom module may need manual calculation for correctness (refer to RNN, LSTM, GRU, Attention as examples).\n",
      "---------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'flops': 46353682032,\n",
       " 'flops_basic': 46264045056,\n",
       " 'params': 51203,\n",
       " 'params_with_aux': 51203}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtools.utils import print_summary\n",
    "model = TSRCNN_small(1, 1, 2)\n",
    "back_frames = torch.randn(1, 3, 360, 640)\n",
    "low_res_frame = torch.randn(1, 3, 360, 640)\n",
    "forward_frames = torch.randn(1, 3, 360, 640)\n",
    "print_summary(model, back_frames, low_res_frame, forward_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------\n",
      "                            Layer (type)    Output shape     Param shape      Param #     FLOPs basic           FLOPs\n",
      "=====================================================================================================================\n",
      "                                 Input *     1x3x360x640\n",
      "                                 Input *     1x3x360x640\n",
      "                                 Input *     1x3x360x640\n",
      "                   layers.0 (Upsample) *    1x9x720x1280                            0               0               0\n",
      "                     layers.1 (Conv2d) *  1x128x710x1270 128x9x11x11+128      139,520 125,689,766,400 125,805,184,000\n",
      "                       layers.2 (ReLU) *  1x128x710x1270                            0               0               0\n",
      "                     layers.3 (Conv2d) *   1x64x708x1268   64x128x3x3+64       73,792  66,188,869,632  66,246,325,248\n",
      "                       layers.4 (ReLU) *   1x64x708x1268                            0               0               0\n",
      "                     layers.5 (Conv2d) *    1x3x702x1262      3x64x7x7+3        9,411   8,334,772,992   8,337,430,764\n",
      "                   layers (Sequential)      1x3x702x1262                            0               0               0\n",
      "                        (TSRCNN_large)      1x3x702x1262                            0               0               0\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Total params: 222,723 (0.8496208190917969 MB)\n",
      "Total params (with aux): 222,723 (0.8496208190917969 MB)\n",
      "    Trainable params: 222,723 (0.8496208190917969 MB)\n",
      "    Non-trainable params: 0 (0.0 MB)\n",
      "Total flops (basic): 200,213,409,024 (200.213409024 billion)\n",
      "Total flops: 200,388,940,012 (200.388940012 billion)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "NOTE:\n",
      "    *: leaf modules\n",
      "    Flops is measured in multiply-adds. Multiply, divide, exp are treated the same for calculation, add is ignored except for bias.\n",
      "    Flops (basic) only calculates for convolution and linear layers (not inlcude bias)\n",
      "    Flops additionally calculates for bias, normalization (BatchNorm, LayerNorm, GroupNorm), RNN (RNN, LSTM, GRU) and attention layers\n",
      "        - activations (e.g. ReLU), operations implemented as functionals (e.g. add in a residual architecture) are not \n",
      "          calculated as they are usually neglectable.\n",
      "        - complex custom module may need manual calculation for correctness (refer to RNN, LSTM, GRU, Attention as examples).\n",
      "---------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'flops': 200388940012,\n",
       " 'flops_basic': 200213409024,\n",
       " 'params': 222723,\n",
       " 'params_with_aux': 222723}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TSRCNN_large(1, 1, 2)\n",
    "print_summary(model, back_frames, low_res_frame, forward_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training procedure is controlled by two main classes of the program MultiTrainer and MultiVideoDataset, and it consists of the following steps:\n",
    "\n",
    "1. Grouping the movies into batches\n",
    "2. Loading of movie batch and creating the dataset\n",
    "3. Splitting the dataset into training and validation sets\n",
    "4. Training the model for n epochs, each epoch consists of:\n",
    "   1. Training\n",
    "   2. Validation\n",
    "\n",
    "### Grouping the movies into batches\n",
    "\n",
    "Ideally we would like to create a single dataset out of all the training movies, but due to very high memory usage during video loading (resulting from some bug in the library) we had to first split videos into batches and perform the whole training procedure on each batch separately. This is not ideal since the model doesn't get to see the whole dataset at once, but it was the only way to make it work. Parameter video_batch_size controls the number of videos in a single batch, on a 32GB RAM machine we were able to load 5 videos at once, but your mileage may vary.\n",
    "\n",
    "### Loading of movie batches and creating the dataset\n",
    "\n",
    "In this step for each movie in batch we load the movie from the disk and read it frame by frame appending each frame to an array, then we utilize sliding window to get sets of frames containing past and future frames for each frame in the movie (except for the first and last frames). The number of past and future frames is controlled by paramaters frames_back and frames_forward. The resulting arrays from each movie are then concatenated into a single array. When dataset is asked for some example it first resizes the frames to the resolution a model is supposed to upscale from and then returns the tuple of tensor containing past and future frames and tensor containing the current frame as well as current frame in an original resolution as a ground truth.\n",
    "\n",
    "### Splitting the dataset into training and validation sets\n",
    "\n",
    "The MultiVideoDataset object is then split into training and validation sets both of which are then wrapped in DataLoader objects\n",
    "\n",
    "### Training\n",
    "\n",
    "During training we first iterate over the training set and for each retrieved batch we predict the output of the model and compare against the ground truth to calculate metrics and loss, we then backpropagate the loss and update the model's weights. Next we iterate over the validation set and do the same but without backpropagation and weight updates. validation loss, and values of metrics get averaged over the whole validation set and reported as the result of the epoch. This process is repeated for n epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code for reference:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset code for reference:\n",
    "\n",
    "```python\n",
    "class MultiVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths: list[str], original_size=(1920, 1080), target_size=(1280, 720), frames_backward=2, frames_forward=2, listener=None, mode: Literal['training', 'inference'] = 'training'):\n",
    "        super().__init__()\n",
    "        self.video_paths = video_paths\n",
    "        self.original_size = original_size\n",
    "        self.target_size = target_size\n",
    "        self.frames_backward = frames_backward\n",
    "        self.frames_forward = frames_forward\n",
    "        self.frame_windows = np.array([])\n",
    "        self.mode = mode\n",
    "        pbar = tqdm(self.video_paths, desc=\"Loading videos\", leave=False)\n",
    "        for i, video_path in enumerate(pbar):\n",
    "            self.load_video(frames_backward, frames_forward, video_path)\n",
    "            sleep(5)  # helps a little to ease the peak system memory usage\n",
    "            if listener:\n",
    "                listener.video_loading_callback((i+1)/len(self.video_paths))\n",
    "\n",
    "    def load_video(self, frames_backward, frames_forward, video_path):\n",
    "        frames = []\n",
    "        reader = ffmpegcv.VideoCaptureNV(video_path, pix_fmt='rgb24') if torch.cuda.is_available() else ffmpegcv.VideoCapture(video_path, pix_fmt='rgb24')\n",
    "        while True:\n",
    "            ret, frame = reader.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        reader.release()\n",
    "        frames = np.array(frames)\n",
    "        windows = sliding_window_view(frames, (frames_backward + frames_forward + 1, *frames[0].shape)).squeeze().astype(np.uint8)\n",
    "        self.frame_windows = np.concatenate([self.frame_windows, windows]) if self.frame_windows.size else windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.frame_windows[idx]\n",
    "        prev_frames = list(frames[:self.frames_backward])\n",
    "        cur_frame = frames[self.frames_backward]\n",
    "        next_frames = list(frames[self.frames_backward + 1:])\n",
    "        for i in range(self.frames_backward):\n",
    "            size = prev_frames[i].shape[0]\n",
    "            if size > self.target_size[1]:\n",
    "                prev_frames[i] = cv2.resize(prev_frames[i], self.target_size, interpolation=cv2.INTER_AREA)\n",
    "            elif size == self.target_size[1]:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Video cannot be smaller than the low resolution size\")\n",
    "            prev_frames[i] = torch.from_numpy(np.transpose(prev_frames[i], (2, 0, 1))).float() / 255.0\n",
    "        prev_frames = torch.concat(prev_frames, dim=0)\n",
    "        size = cur_frame.shape[0]\n",
    "        if self.mode == 'training':\n",
    "            if size > self.original_size[1]:\n",
    "                high_res_frame = cv2.resize(cur_frame, self.original_size, interpolation=cv2.INTER_AREA)\n",
    "            elif size == self.original_size[1]:\n",
    "                high_res_frame = cur_frame\n",
    "            else:\n",
    "                raise Exception(\"Video cannot be smaller than the high resolution except for the inference mode\")\n",
    "            high_res_frame = torch.from_numpy(np.transpose(high_res_frame, (2, 0, 1))).float() / 255.0\n",
    "        if size > self.target_size[1]:\n",
    "            low_res_frame = cv2.resize(cur_frame, self.target_size, interpolation=cv2.INTER_AREA)\n",
    "        elif size == self.target_size[1]:\n",
    "            low_res_frame = cur_frame\n",
    "        else:\n",
    "            raise Exception(\"Video cannot be smaller than the low resolution size\")\n",
    "        low_res_frame = torch.from_numpy(np.transpose(low_res_frame, (2, 0, 1))).float() / 255.0\n",
    "        for i in range(self.frames_forward):\n",
    "            size = next_frames[i].shape[0]\n",
    "            if size > self.target_size[1]:\n",
    "                next_frames[i] = cv2.resize(next_frames[i], self.target_size, interpolation=cv2.INTER_AREA)\n",
    "            elif size == self.target_size[1]:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Video cannot be smaller than the low resolution size\")\n",
    "            next_frames[i] = torch.from_numpy(np.transpose(next_frames[i], (2, 0, 1))).float() / 255.0\n",
    "        next_frames = torch.concat(next_frames, dim=0)\n",
    "        return ((prev_frames, low_res_frame, next_frames),  high_res_frame) if self.mode == 'training' else (prev_frames, low_res_frame, next_frames)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer code for reference:\n",
    "\n",
    "```python\n",
    "class MultiTrainer:\n",
    "    def __init__(self, device='auto', original_size=(1920, 1080), target_size=(1280, 720), learning_rate: float = 0.001, optimizer: Literal['AdamW', 'Adagrad', 'SGD'] = 'AdamW', loss: Literal['MSE', 'PNSR', 'DSSIM'] = 'MSE', frames_backward=2, frames_forward=2, model=None):\n",
    "        if device == 'auto':\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.criterion = LOSS[loss]()\n",
    "        self.original_size = original_size\n",
    "        self.target_size = target_size\n",
    "        self.base_videos = int(model.split('_')[3].split('v')[0]) if model[:3] != 'new' else 0\n",
    "        upscale_factor = original_size[1]/target_size[1]\n",
    "        if model == 'new TSRCNN_large':\n",
    "            self.model = TSRCNN_large(upscale_factor=upscale_factor, frames_backward=frames_backward, frames_forward=frames_forward).to(device)\n",
    "            self.size = 'large'\n",
    "        elif model == 'new TSRCNN_small':\n",
    "            self.model = TSRCNN_small(upscale_factor=upscale_factor, frames_backward=frames_backward, frames_forward=frames_forward).to(device)\n",
    "            self.size = 'small'\n",
    "        else:\n",
    "            if model.split('_')[0] == 'models/small' or model.split('_')[0] == 'small':\n",
    "                if model[:6] != 'models':\n",
    "                    model = 'models/' + model\n",
    "                self.model = TSRCNN_small.load(model, frames_backward=frames_backward, frames_forward=frames_forward, upscale_factor=upscale_factor).to(device)\n",
    "                self.size = 'small'\n",
    "            else:\n",
    "                if model[:6] != 'models':\n",
    "                    model = 'models/' + model\n",
    "                self.model = TSRCNN_large.load(model, frames_backward=frames_backward, frames_forward=frames_forward, upscale_factor=upscale_factor).to(device)\n",
    "                self.size = 'large'\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = OPTIMIZER[optimizer](self.model.parameters(), lr=learning_rate)\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'epoch_loss': [], 'train_metrics': {'PSNR': [],\n",
    "                                                                                              'SSIM': []}, 'val_metrics': {'PSNR': [], 'SSIM': []}, 'epoch_metrics': {'PSNR': [], 'SSIM': []}}\n",
    "        self.listener: SimpleListener = None\n",
    "        self.last_loss = None\n",
    "        self.save_interval = 1\n",
    "        self.dataset_format = MultiVideoDataset\n",
    "        self.frames_backward = frames_backward\n",
    "        self.frames_forward = frames_forward\n",
    "\n",
    "    def _log_params(self, parameters: Dict):\n",
    "        for key, value in parameters.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "    def train_model(self, video_files: list[str] = ['video.mp4'], num_epochs=15, batch_size=2, video_batch_size=3) -> str:\n",
    "        mlflow.set_experiment(\"temporal_super_resolution_experiment\")\n",
    "        video_batches = [video_files[i:i+video_batch_size] if i+video_batch_size <\n",
    "                         len(video_files) else video_files[i:] for i in range(0, len(video_files), video_batch_size)]\n",
    "        global_training = tqdm(enumerate(video_batches), total=len(video_batches), desc='Global Training')\n",
    "        for i, video_files in global_training:\n",
    "            dataset = self.dataset_format(video_files, original_size=self.original_size, target_size=self.target_size,\n",
    "                                          frames_backward=self.frames_backward, frames_forward=self.frames_forward, listener=self.listener)\n",
    "\n",
    "            train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "            train_size = len(train_dataset)\n",
    "            val_size = len(val_dataset)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "            self.run_name = f\"{self.size}_{self.target_size[1]}_{self.original_size[1]}_{self.base_videos + i*video_batch_size +\n",
    "                                                                                         len(video_files)}videos_{self.optimizer.__class__.__name__}opt', f'_{self.criterion.__class__.__name__}loss_{self.frames_backward}fb_{self.frames_forward}ff_{num_epochs}ep\"\n",
    "            with mlflow.start_run(run_name=self.run_name):\n",
    "                self._log_params({\"video_files\": video_files,\n",
    "                                  \"already_trained_on_videos\": i*video_batch_size,\n",
    "                                  \"num_epochs\": num_epochs,\n",
    "                                  \"train_size\": train_size,\n",
    "                                  \"val_size\": val_size,\n",
    "                                  \"batch_size\": batch_size,\n",
    "                                  \"original_size\": self.original_size,\n",
    "                                  \"target_size\": self.target_size,\n",
    "                                  \"optimizer\": self.optimizer.__class__.__name__,\n",
    "                                  \"learning_rate\": self.learning_rate,\n",
    "                                  \"criterion\": self.criterion.__class__.__name__,\n",
    "                                  \"forward_frames\": self.frames_forward,\n",
    "                                  \"backward_frames\": self.frames_backward})\n",
    "\n",
    "                pbar = tqdm(range(1, num_epochs + 1), desc='Training',\n",
    "                            unit='epoch', postfix={'loss': 'inf'})\n",
    "                for epoch in pbar:\n",
    "                    self.single_epoch(train_loader, val_loader, epoch)\n",
    "                    if self.listener is not None:\n",
    "                        self.listener.epoch_callback(epoch/num_epochs, history=self.history)\n",
    "                    if epoch % self.save_interval == 0:\n",
    "                        save_path = ''.join([f'models/{self.size}_{self.target_size[1]}_{self.original_size[1]}_{self.base_videos + i*video_batch_size +\n",
    "                                                                                                                 len(video_files)}videos_{self.optimizer.__class__.__name__}opt', f'_{self.criterion.__class__.__name__}loss_{self.frames_backward}fb_{self.frames_forward}ff_{num_epochs}ep_{epoch}.pt'])\n",
    "\n",
    "                        self.model.eval()\n",
    "                        self.save(save_path)\n",
    "                        mlflow.log_artifact(save_path, artifact_path=\"models\")\n",
    "                        mlflow.pytorch.log_model(self.model, artifact_path=save_path.split('.')[0])\n",
    "                        self.model.to(self.device)\n",
    "                    pbar.set_postfix({'loss': self.epoch_loss, 'PSNR': self.epoch_psnr, 'SSIM': self.epoch_ssim})\n",
    "                    mlflow.log_metric(\"epoch_loss\", self.epoch_loss, step=epoch)\n",
    "                    mlflow.log_metric(\"epoch_PSNR\", self.epoch_psnr, step=epoch)\n",
    "                    mlflow.log_metric(\"epoch_SSIM\", self.epoch_ssim, step=epoch)\n",
    "                self.save('_'.join(save_path.split('_')[:-1]) + '_final.pt')\n",
    "                self.model.to(self.device)\n",
    "        return '_'.join(save_path.split('_')[:-1]) + '_final.pt'\n",
    "\n",
    "    def train_batch(self, prev_frames, low_res_frame, next_frames, high_res_frame):\n",
    "        prev_frames = prev_frames.to(self.device)\n",
    "        low_res_frame = low_res_frame.to(self.device)\n",
    "        next_frames = next_frames.to(self.device)\n",
    "        high_res_frame = high_res_frame.to(self.device)\n",
    "        if self.size == 'small':\n",
    "            high_res_frame = high_res_frame[:, :, 6:-6, 6:-6]\n",
    "        if self.size == 'large':\n",
    "            high_res_frame = high_res_frame[:, :, 9:-9, 9:-9]\n",
    "        self.optimizer.zero_grad()\n",
    "        pred_high_res_frame = self.model(prev_frames, low_res_frame, next_frames)\n",
    "        loss = self.criterion(pred_high_res_frame, high_res_frame)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        self.history['train_loss'].append(loss)\n",
    "        psnr = PSNR(pred_high_res_frame, high_res_frame).item()\n",
    "        ssim = SSIM(pred_high_res_frame, high_res_frame).item()\n",
    "        self.history['train_metrics']['PSNR'].append(psnr)\n",
    "        self.history['train_metrics']['SSIM'].append(ssim)\n",
    "        return loss, (psnr, ssim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_batch(self, prev_frames, low_res_frame, next_frames, high_res_frame):\n",
    "        prev_frames = prev_frames.to(self.device)\n",
    "        low_res_frame = low_res_frame.to(self.device)\n",
    "        next_frames = next_frames.to(self.device)\n",
    "        high_res_frame = high_res_frame.to(self.device)\n",
    "        if self.size == 'small':\n",
    "            high_res_frame = high_res_frame[:, :, 6:-6, 6:-6]\n",
    "        if self.size == 'large':\n",
    "            high_res_frame = high_res_frame[:, :, 9:-9, 9:-9]\n",
    "        pred_high_res_frame = self.model(prev_frames, low_res_frame, next_frames)\n",
    "        loss = self.criterion(pred_high_res_frame, high_res_frame).item()\n",
    "        self.history['val_loss'].append(loss)\n",
    "        psnr = PSNR(pred_high_res_frame, high_res_frame).item()\n",
    "        ssim = SSIM(pred_high_res_frame, high_res_frame).item()\n",
    "        self.history['val_metrics']['PSNR'].append(psnr)\n",
    "        self.history['val_metrics']['SSIM'].append(ssim)\n",
    "        return loss, (psnr, ssim)\n",
    "\n",
    "    def single_epoch(self, train_loader, val_loader, epoch):\n",
    "        self.model.train()\n",
    "        train_losses = []\n",
    "        train_psnr = []\n",
    "        train_ssim = []\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', unit='batch', leave=True)\n",
    "        for i, ((prev_frames, low_res_frame, next_frames), high_res_frame) in enumerate(train_pbar):\n",
    "            loss, metrics = self.train_batch(prev_frames, low_res_frame, next_frames, high_res_frame)\n",
    "            if self.listener is not None:\n",
    "                self.listener.train_batch_callback((i+1)/len(train_loader), self.history)\n",
    "            train_losses.append(loss)\n",
    "            train_psnr.append(metrics[0])\n",
    "            train_ssim.append(metrics[1])\n",
    "            train_pbar.set_postfix({'train_loss': loss, 'train_PSNR': metrics[0], 'train_SSIM': metrics[1]})\n",
    "\n",
    "        self.model.eval()\n",
    "        val_losses = []\n",
    "        val_psnr = []\n",
    "        val_ssim = []\n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch}', unit='batch', leave=True)\n",
    "        for i, ((prev_frames, low_res_frame, next_frames), high_res_frame) in enumerate(val_pbar):\n",
    "            loss, metrics = self.val_batch(prev_frames, low_res_frame, next_frames, high_res_frame)\n",
    "            if self.listener is not None:\n",
    "                self.listener.val_batch_callback((i+1)/len(val_loader), self.history)\n",
    "            val_losses.append(loss)\n",
    "            val_psnr.append(metrics[0])\n",
    "            val_ssim.append(metrics[1])\n",
    "            val_pbar.set_postfix({'val_loss': loss, 'val_PSNR': metrics[0], 'val_SSIM': metrics[1]})\n",
    "        self.epoch_loss = sum(val_losses) / len(val_losses)\n",
    "        self.epoch_psnr = sum(val_psnr) / len(val_psnr)\n",
    "        self.epoch_ssim = sum(val_ssim) / len(val_ssim)\n",
    "        self.history['epoch_loss'].append(self.epoch_loss)\n",
    "        self.history['epoch_metrics']['PSNR'].append(self.epoch_psnr)\n",
    "        self.history['epoch_metrics']['SSIM'].append(self.epoch_ssim)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "        self.model.to(self.device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our program also uses mlflow to log the training process and save the models as well as Simple GUI created in streamlit which allows both to train and test models, and is the recommended way to run the program:\n",
    "![train](resources/train.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI allows to choose all the necessary hyperparameters, if you have an already trained model in models/ directory you can choose it to \"uptrain\" it, otherwise you can train a new model from scratch. This page expects that the videos you provide will be at least the size of High resolution parameter, and they need to have 16:9 proportions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you click start training progress bars and graphs will appear allowing you to monitor the training process. Program generates 3x3 graph grid, where first row concerns the subsequent batches of training, the second batches of validation, and the third epochs. The first column shows the value of PSNR metric, the second column shows the value of SSIM metric, and the third column shows the value of currently chosen loss function. The graphs are updated after each batch and epoch. Epochs loss and metrics are also logged to the mlflow and can be retrieved later. MLflow saves also the model after each epoch as well as parameters used in training.\n",
    "\n",
    "![plots](resources/Adam_PSNR_252_10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Before we go to the evaluation procedure let's describe metrics and loss functions we used:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSNR (Peak Signal-to-Noise Ratio):**\n",
    "\n",
    "$$\n",
    "\\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{\\text{MAX}^2}{\\text{MSE}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\text{MAX}$ is the maximum possible pixel value of the image.\n",
    "- $\\text{MSE}$ is the Mean Squared Error between images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSIM (Structural Similarity Index):**\n",
    "$$\\text{SSIM}(x, y) = \\frac{(2\\mu*x\\mu_y + C_1)(2\\sigma*{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mu_x, \\mu_y$ are the mean intensities of $x$ and $y$.\n",
    "- $\\sigma_x^2, \\sigma_y^2$ are the variances of $x$ and $y$.\n",
    "- $\\sigma\\_{xy}$ is the covariance between $x$ and $y$.\n",
    "- $C_1 = (k_1 L)^2$ and $C_2 = (k_2 L)^2$ are constants to stabilize the division (typically $k_1 = 0.01$, $k_2 = 0.03$, and $L$ is the dynamic range of pixel values).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these two metrics we introduce two loss functions:\n",
    "\n",
    "- PNSR (Peak Noise-to-Signal Ratio): $\\frac{1}{\\text{PSNR}}$\n",
    "- DSSIM (Structural Dissimilarity Index): $1 - \\text{SSIM}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation works similarly to the training procedure with only difference being that only a single video is being loaded and after the model upscales each frame it is being written to the new high resolution video\n",
    "Evaluation can be run with in test mode which expect high resolution video input and will report values of metrics just during training, or in inference mode which expects low resolution video input and will generate upscaled video\n",
    "\n",
    "![test](resources/test.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation code for reference:\n",
    "\n",
    "```python\n",
    "class Upscaler:\n",
    "    def __init__(self, model_path, device='auto', original_size=(1920, 1080), target_size=(1280, 720), listener=None, frames_backward=2, frames_forward=2, mode: Literal['test', 'inference'] = 'test'):\n",
    "        if device == 'auto':\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.original_size = original_size\n",
    "        self.target_size = target_size\n",
    "        upscale_factor = original_size[0] / target_size[0]\n",
    "        self.size = model_path.split('_')[0].split('/')[1]\n",
    "        if self.size == 'large':\n",
    "            self.model = TSRCNN_large.load(model_path, frames_backward=frames_backward, frames_forward=frames_forward, upscale_factor=upscale_factor).to(device)\n",
    "        else:\n",
    "            self.model = TSRCNN_small.load(model_path, frames_backward=frames_backward, frames_forward=frames_forward, upscale_factor=upscale_factor).to(device)\n",
    "        self.dataset_format = MultiVideoDataset\n",
    "        self.listener: SimpleListener = listener\n",
    "        self.run_name = f\"{target_size[1]}p -> {original_size[1]}p {frames_backward}fb{frames_forward}ff TSR {mode}\"\n",
    "        self.history = {'test_metrics': {'PSNR': [], 'SSIM': []}, 'cubic_metrics': {'PSNR': [], 'SSIM': []}}\n",
    "        self.mode = mode\n",
    "        self.frames_backward = frames_backward\n",
    "        self.frames_forward = frames_forward\n",
    "\n",
    "    def _log_params(self, parameters: Dict):\n",
    "        for key, value in parameters.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "    def upscale(self, video_file, fps=60.0, video_path_out=\"output.mp4\"):\n",
    "        writer = ffmpegcv.VideoWriterNV(file=video_path_out, codec='h264_nvenc', fps=fps, preset='p7', pix_fmt='rgb24') if self.device == 'cuda' else ffmpegcv.VideoWriter(\n",
    "            file=video_path_out, codec='h264', fps=60.0, preset='p7', pix_fmt='rgb24')\n",
    "        cubic = ffmpegcv.VideoWriterNV(file=\"bicubic.mp4\", codec='h264_nvenc', fps=fps, preset='p7', pix_fmt='rgb24') if self.device == 'cuda' else ffmpegcv.VideoWriter(\n",
    "            file=\"bicubic.mp4\", codec='h264', fps=60.0, preset='p7', pix_fmt='rgb24')\n",
    "        test_dataset = self.dataset_format(video_paths=[video_file], original_size=self.original_size,\n",
    "                                           target_size=self.target_size, frames_backward=self.frames_backward, frames_forward=self.frames_forward, mode='inference' if self.mode == 'inference' else 'training')\n",
    "        num_frames = len(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "        mlflow.set_experiment(\"temporal_super_resolution_experiment\")\n",
    "        with mlflow.start_run(run_name=self.run_name):\n",
    "            self._log_params({\"video_file\": video_file,\n",
    "                              \"test_size\": num_frames,\n",
    "                              \"original_size\": self.original_size,\n",
    "                              \"target_size\": self.target_size})\n",
    "        self.model.eval()\n",
    "        test_psnrs = []\n",
    "        test_ssim = []\n",
    "        cubic_psnrs = []\n",
    "        cubic_ssim = []\n",
    "        test_pbar = tqdm(test_loader, total=num_frames, unit='frame')\n",
    "        if self.mode == 'test':\n",
    "            for i, ((prev_frames, low_res_frame, next_frames), high_res_frame) in enumerate(test_pbar):\n",
    "                pred_frame, metrics = self.test_batch(prev_frames, low_res_frame, next_frames, high_res_frame)\n",
    "                test_psnrs.append(metrics[0])\n",
    "                test_ssim.append(metrics[1])\n",
    "                test_pbar.set_postfix({'test_psnr': metrics[0], 'test_ssim': metrics[1]})\n",
    "                self.history['test_metrics']['PSNR'].append(metrics[0])\n",
    "                self.history['test_metrics']['SSIM'].append(metrics[1])\n",
    "                if self.listener is not None:\n",
    "                    self.listener.test_batch_callback((i+1)/len(test_loader), self.history)\n",
    "                cubic_frame = nn.functional.interpolate(low_res_frame, self.original_size[::-1], mode='bicubic')\n",
    "                cubic_metrics = (PSNR(cubic_frame, high_res_frame).item(), SSIM(cubic_frame, high_res_frame).item())\n",
    "                cubic_psnrs.append(cubic_metrics[0])\n",
    "                cubic_ssim.append(cubic_metrics[1])\n",
    "                self.history['cubic_metrics']['PSNR'].append(cubic_metrics[0])\n",
    "                self.history['cubic_metrics']['SSIM'].append(cubic_metrics[1])\n",
    "                cubic_frame = (np.clip(cubic_frame.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)*255).astype(np.uint8)\n",
    "                cubic.write(cubic_frame)\n",
    "                out = pred_frame.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                out = np.clip(out, 0, 1)\n",
    "                out = (out * 255).astype(np.uint8)\n",
    "                writer.write(out)\n",
    "        else:\n",
    "            for i, (prev_frames, low_res_frame, next_frames) in enumerate(test_pbar):\n",
    "                if self.listener is not None:\n",
    "                    self.listener.test_batch_callback((i+1)/len(test_loader), None)\n",
    "                pred_frame = self.upscale_batch(prev_frames, low_res_frame, next_frames, high_res_frame)\n",
    "                cubic_frame = nn.functional.interpolate(low_res_frame, self.original_size[::-1], mode='bicubic')\n",
    "                cubic_frame = (np.clip(cubic_frame.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)*255).astype(np.uint8)\n",
    "                cubic.write(cubic_frame)\n",
    "                test_pbar.set_postfix({'frame': i})\n",
    "                out = pred_frame.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                out = np.clip(out, 0, 1)\n",
    "                out = (out * 255).astype(np.uint8)\n",
    "                writer.write(out)\n",
    "        cv2.destroyAllWindows()\n",
    "        writer.release()\n",
    "        cubic.release()\n",
    "        if self.mode == 'test':\n",
    "            final_psnr = sum(test_psnrs)/len(test_psnrs)\n",
    "            final_ssim = sum(test_ssim)/len(test_ssim)\n",
    "            final_cubic_psnr = sum(cubic_psnrs)/len(cubic_psnrs)\n",
    "            final_cubic_ssim = sum(cubic_ssim)/len(cubic_ssim)\n",
    "            if self.listener is not None:\n",
    "                self.listener.final_loss_callback(final_psnr, final_ssim, final_cubic_psnr, final_cubic_ssim)\n",
    "            mlflow.log_metric(\"test_psnr\", final_psnr)\n",
    "            mlflow.log_metric(\"test_ssim\", final_ssim)\n",
    "            return final_psnr, final_ssim, final_cubic_psnr, final_cubic_ssim\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_batch(self, prev_frames, low_res_frame, next_frames, high_res_frame):\n",
    "        prev_frames = prev_frames.to(self.device)\n",
    "        low_res_frame = low_res_frame.to(self.device)\n",
    "        next_frames = next_frames.to(self.device)\n",
    "        high_res_frame = high_res_frame.to(self.device)\n",
    "        if self.size == 'small':\n",
    "            high_res_frame = high_res_frame[:, :, 6:-6, 6:-6]\n",
    "        if self.size == 'large':\n",
    "            high_res_frame = high_res_frame[:, :, 9:-9, 9:-9]\n",
    "        pred_high_res_frame = self.model(prev_frames, low_res_frame, next_frames)\n",
    "        psnr = PSNR(pred_high_res_frame, high_res_frame).item()\n",
    "        ssim = SSIM(pred_high_res_frame, high_res_frame).item()\n",
    "        return pred_high_res_frame, (psnr, ssim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def upscale_batch(self, prev_frames, low_res_frame, next_frames):\n",
    "        prev_frames = prev_frames.to(self.device)\n",
    "        low_res_frame = low_res_frame.to(self.device)\n",
    "        next_frames = next_frames.to(self.device)\n",
    "        pred_high_res_frame = self.model(prev_frames, low_res_frame, next_frames)\n",
    "        return pred_high_res_frame\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Results\n",
    "\n",
    "We wanted to test 3 optimizers AdamW, Adagrad, and SGD, and 3 loss functions MSE, PNSR, and DSSIM, additionally we wanted to test the larger version of the model, however due to the computational limitations we couldn't train each model on the whole data so we started with a smaller dataset of 36 videos (~10,000 frames) and uptrained only the models which got good results. In this first turn on 36 datasets and 10 epochs we trained in total 6 models:\n",
    "\n",
    "- small model with AdamW optimizer and MSE loss\n",
    "- small model with AdamW optimizer and PNSR loss\n",
    "- small model with AdamW optimizer and DSSIM loss\n",
    "- large model with AdamW optimizer and MSE loss\n",
    "- small model with Adagrad optimizer and MSE loss\n",
    "- small model with Adagrad optimizer and PNSR loss\n",
    "- small model with SGD optimizer and MSE loss\n",
    "\n",
    "**Hyper paramters**\\\n",
    "For each model we used 0.0001 learning rate as it is a popular choice and a single past and future frame. We tried using 2 frames but the results were not good enough to justify the increased computational demand. Each model was trained ans tested on 360->730p upscaling, because smaller image size allowed us to train the models faster. Batch size was set to 8 which was dictated by the VRAM capacity of the GPU.\n",
    "\n",
    "**Training and inference times**\\\n",
    "Training of a single model in this configuration took around 2.5h this was also true for larger model trained with smaller number of epochs, and later when we training models on 108 videos with 3 epochs the time was also similar. Inference with smaller model takes around 50s per video, 6 frames per second, and with larger model around 75s per video, 4 frames per second.\n",
    "\n",
    "Already in the training phase model trained with SGD optimizer gave results so poor that we resigned from training it on the two remaining losses. Adagrad's performance was similarly poor and we skipped training it on DSSIM loss. As mentioned before the large model was trained only for 3 epochs instead of 10 due to computational limitations which resulted in underfitting. After training we evaluated the models on 10 videos from the dataset (none of which were used in training) for each of them we calculate average values of metrics all frames, Since cubic interpolation is our baseline we also calculated the same metrics for it calculated the difference between the model metrics and cubic metrics. Additionally since having two separate metrics made it difficult to compare models we introduced an additional metric QM (Quality Measure) defined as: a geometric mean of PSNR and DSSIM.\n",
    "$$\\text{Quality Measure (QM)} = \\sqrt{\\text{PNSR}*\\text{DSSIM}}$$\n",
    "To compare models we also averaged the results over all 10 testing videos and calulated a score as number of videos in which given model outperformed the cubic interpolation. The results are presented in the table below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100.mp4_psnr</th>\n",
       "      <th>100.mp4_qm</th>\n",
       "      <th>100.mp4_ssim</th>\n",
       "      <th>1000.mp4_psnr</th>\n",
       "      <th>1000.mp4_qm</th>\n",
       "      <th>1000.mp4_ssim</th>\n",
       "      <th>200.mp4_psnr</th>\n",
       "      <th>200.mp4_qm</th>\n",
       "      <th>200.mp4_ssim</th>\n",
       "      <th>...</th>\n",
       "      <th>800.mp4_psnr</th>\n",
       "      <th>800.mp4_qm</th>\n",
       "      <th>800.mp4_ssim</th>\n",
       "      <th>900.mp4_psnr</th>\n",
       "      <th>900.mp4_qm</th>\n",
       "      <th>900.mp4_ssim</th>\n",
       "      <th>average_psnr</th>\n",
       "      <th>average_ssim</th>\n",
       "      <th>average_qm</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_360_720_36videos_AdamWopt_MSELossloss_1f...</td>\n",
       "      <td>-3.734211</td>\n",
       "      <td>-0.300641</td>\n",
       "      <td>-0.006046</td>\n",
       "      <td>-0.452009</td>\n",
       "      <td>-0.297245</td>\n",
       "      <td>-0.089115</td>\n",
       "      <td>0.450476</td>\n",
       "      <td>0.048059</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115380</td>\n",
       "      <td>-0.022719</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>0.430492</td>\n",
       "      <td>0.071991</td>\n",
       "      <td>0.012013</td>\n",
       "      <td>-0.963966</td>\n",
       "      <td>-0.026104</td>\n",
       "      <td>-0.154099</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small_360_720_36videos_Adagradopt_MSELossloss_...</td>\n",
       "      <td>-6.165242</td>\n",
       "      <td>-0.527323</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-4.967614</td>\n",
       "      <td>-0.918187</td>\n",
       "      <td>-0.169263</td>\n",
       "      <td>-4.230602</td>\n",
       "      <td>-0.824915</td>\n",
       "      <td>-0.159152</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.170785</td>\n",
       "      <td>-0.433223</td>\n",
       "      <td>-0.084580</td>\n",
       "      <td>-2.959594</td>\n",
       "      <td>-0.578530</td>\n",
       "      <td>-0.111705</td>\n",
       "      <td>-4.707401</td>\n",
       "      <td>-0.111740</td>\n",
       "      <td>-0.721222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small_360_720_36videos_Adagradopt_PNSRloss_1fb...</td>\n",
       "      <td>-5.767887</td>\n",
       "      <td>-0.491491</td>\n",
       "      <td>-0.016400</td>\n",
       "      <td>-4.046040</td>\n",
       "      <td>-0.665161</td>\n",
       "      <td>-0.108710</td>\n",
       "      <td>-3.625270</td>\n",
       "      <td>-0.693373</td>\n",
       "      <td>-0.131711</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761099</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.067251</td>\n",
       "      <td>-2.554079</td>\n",
       "      <td>-0.485140</td>\n",
       "      <td>-0.091525</td>\n",
       "      <td>-3.921213</td>\n",
       "      <td>-0.082794</td>\n",
       "      <td>-0.569042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small_360_720_36videos_AdamWopt_DSSIMloss_1fb_...</td>\n",
       "      <td>-4.487203</td>\n",
       "      <td>-0.339390</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.177847</td>\n",
       "      <td>-0.037914</td>\n",
       "      <td>-0.018717</td>\n",
       "      <td>0.310632</td>\n",
       "      <td>0.070956</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103374</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.412186</td>\n",
       "      <td>0.097714</td>\n",
       "      <td>0.021601</td>\n",
       "      <td>-0.949440</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.082781</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>small_360_720_36videos_AdamWopt_MSELossloss_1f...</td>\n",
       "      <td>-6.452513</td>\n",
       "      <td>-0.500426</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>0.173088</td>\n",
       "      <td>-0.048839</td>\n",
       "      <td>-0.022369</td>\n",
       "      <td>0.308065</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070231</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.294060</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>-1.333145</td>\n",
       "      <td>-0.007172</td>\n",
       "      <td>-0.125245</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>small_360_720_36videos_AdamWopt_PNSRloss_1fb_1...</td>\n",
       "      <td>-1.699418</td>\n",
       "      <td>-0.134431</td>\n",
       "      <td>-0.002420</td>\n",
       "      <td>-0.463035</td>\n",
       "      <td>-0.281817</td>\n",
       "      <td>-0.083579</td>\n",
       "      <td>0.523455</td>\n",
       "      <td>0.058180</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133572</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.514038</td>\n",
       "      <td>0.077649</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>-0.700565</td>\n",
       "      <td>-0.029648</td>\n",
       "      <td>-0.145092</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>small_360_720_36videos_SGDopt_MSELossloss_1fb_...</td>\n",
       "      <td>-18.034602</td>\n",
       "      <td>-1.647082</td>\n",
       "      <td>-0.056695</td>\n",
       "      <td>-9.682540</td>\n",
       "      <td>-1.876009</td>\n",
       "      <td>-0.359083</td>\n",
       "      <td>-9.181472</td>\n",
       "      <td>-1.794721</td>\n",
       "      <td>-0.345958</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.394648</td>\n",
       "      <td>-1.161814</td>\n",
       "      <td>-0.210196</td>\n",
       "      <td>-8.182921</td>\n",
       "      <td>-1.561708</td>\n",
       "      <td>-0.295159</td>\n",
       "      <td>-11.706562</td>\n",
       "      <td>-0.240023</td>\n",
       "      <td>-1.704240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  100.mp4_psnr  \\\n",
       "0  large_360_720_36videos_AdamWopt_MSELossloss_1f...     -3.734211   \n",
       "1  small_360_720_36videos_Adagradopt_MSELossloss_...     -6.165242   \n",
       "2  small_360_720_36videos_Adagradopt_PNSRloss_1fb...     -5.767887   \n",
       "3  small_360_720_36videos_AdamWopt_DSSIMloss_1fb_...     -4.487203   \n",
       "4  small_360_720_36videos_AdamWopt_MSELossloss_1f...     -6.452513   \n",
       "5  small_360_720_36videos_AdamWopt_PNSRloss_1fb_1...     -1.699418   \n",
       "6  small_360_720_36videos_SGDopt_MSELossloss_1fb_...    -18.034602   \n",
       "\n",
       "   100.mp4_qm  100.mp4_ssim  1000.mp4_psnr  1000.mp4_qm  1000.mp4_ssim  \\\n",
       "0   -0.300641     -0.006046      -0.452009    -0.297245      -0.089115   \n",
       "1   -0.527323     -0.017874      -4.967614    -0.918187      -0.169263   \n",
       "2   -0.491491     -0.016400      -4.046040    -0.665161      -0.108710   \n",
       "3   -0.339390     -0.000088       0.177847    -0.037914      -0.018717   \n",
       "4   -0.500426     -0.002212       0.173088    -0.048839      -0.022369   \n",
       "5   -0.134431     -0.002420      -0.463035    -0.281817      -0.083579   \n",
       "6   -1.647082     -0.056695      -9.682540    -1.876009      -0.359083   \n",
       "\n",
       "   200.mp4_psnr  200.mp4_qm  200.mp4_ssim  ...  800.mp4_psnr  800.mp4_qm  \\\n",
       "0      0.450476    0.048059      0.002965  ...     -0.115380   -0.022719   \n",
       "1     -4.230602   -0.824915     -0.159152  ...     -2.170785   -0.433223   \n",
       "2     -3.625270   -0.693373     -0.131711  ...     -1.761099   -0.347364   \n",
       "3      0.310632    0.070956      0.015380  ...      0.103374    0.041477   \n",
       "4      0.308065    0.034897      0.002735  ...      0.070231    0.007311   \n",
       "5      0.523455    0.058180      0.004271  ...      0.133572    0.008140   \n",
       "6     -9.181472   -1.794721     -0.345958  ...     -6.394648   -1.161814   \n",
       "\n",
       "   800.mp4_ssim  900.mp4_psnr  900.mp4_qm  900.mp4_ssim  average_psnr  \\\n",
       "0     -0.004397      0.430492    0.071991      0.012013     -0.963966   \n",
       "1     -0.084580     -2.959594   -0.578530     -0.111705     -4.707401   \n",
       "2     -0.067251     -2.554079   -0.485140     -0.091525     -3.921213   \n",
       "3      0.011194      0.412186    0.097714      0.021601     -0.949440   \n",
       "4      0.000445      0.294060    0.054356      0.010020     -1.333145   \n",
       "5     -0.001122      0.514038    0.077649      0.011440     -0.700565   \n",
       "6     -0.210196     -8.182921   -1.561708     -0.295159    -11.706562   \n",
       "\n",
       "   average_ssim  average_qm  score  \n",
       "0     -0.026104   -0.154099    0.5  \n",
       "1     -0.111740   -0.721222    0.0  \n",
       "2     -0.082794   -0.569042    0.0  \n",
       "3     -0.002913   -0.082781    0.6  \n",
       "4     -0.007172   -0.125245    0.4  \n",
       "5     -0.029648   -0.145092    0.6  \n",
       "6     -0.240023   -1.704240    0.0  \n",
       "\n",
       "[7 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('resources/results1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately in this first stage none of the trained models outperformed the interpolation on average, however AdamW models trained on DSSIM and PNSR outperformed the cubic interpolation on 6 out of 10 videos so we selected them for the next stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next stage we uptrained the selected models with 108 more videos and 3 more epochs, the results are presented in the table below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100.mp4_psnr</th>\n",
       "      <th>100.mp4_qm</th>\n",
       "      <th>100.mp4_ssim</th>\n",
       "      <th>1000.mp4_psnr</th>\n",
       "      <th>1000.mp4_qm</th>\n",
       "      <th>1000.mp4_ssim</th>\n",
       "      <th>200.mp4_psnr</th>\n",
       "      <th>200.mp4_qm</th>\n",
       "      <th>200.mp4_ssim</th>\n",
       "      <th>...</th>\n",
       "      <th>800.mp4_psnr</th>\n",
       "      <th>800.mp4_qm</th>\n",
       "      <th>800.mp4_ssim</th>\n",
       "      <th>900.mp4_psnr</th>\n",
       "      <th>900.mp4_qm</th>\n",
       "      <th>900.mp4_ssim</th>\n",
       "      <th>average_psnr</th>\n",
       "      <th>average_ssim</th>\n",
       "      <th>average_qm</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_360_720_144videos_AdamWopt_DSSIMloss_1fb...</td>\n",
       "      <td>-7.910640</td>\n",
       "      <td>-0.612011</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>0.885894</td>\n",
       "      <td>0.105123</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.221640</td>\n",
       "      <td>0.073816</td>\n",
       "      <td>0.019194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071826</td>\n",
       "      <td>0.060105</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.352403</td>\n",
       "      <td>0.103225</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>-0.626638</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>-0.012920</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small_360_720_144videos_AdamWopt_PNSRloss_1fb_...</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>0.752685</td>\n",
       "      <td>0.056118</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>0.967591</td>\n",
       "      <td>0.136301</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529495</td>\n",
       "      <td>0.073506</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.798743</td>\n",
       "      <td>0.128736</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.855944</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.084644</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  100.mp4_psnr  \\\n",
       "0  small_360_720_144videos_AdamWopt_DSSIMloss_1fb...     -7.910640   \n",
       "1  small_360_720_144videos_AdamWopt_PNSRloss_1fb_...      0.397138   \n",
       "\n",
       "   100.mp4_qm  100.mp4_ssim  1000.mp4_psnr  1000.mp4_qm  1000.mp4_ssim  \\\n",
       "0   -0.612011     -0.000482       0.885894     0.105123       0.009631   \n",
       "1    0.027760     -0.000419       0.752685     0.056118      -0.003395   \n",
       "\n",
       "   200.mp4_psnr  200.mp4_qm  200.mp4_ssim  ...  800.mp4_psnr  800.mp4_qm  \\\n",
       "0      0.221640    0.073816      0.019194  ...      0.071826    0.060105   \n",
       "1      0.967591    0.136301      0.018012  ...      0.529495    0.073506   \n",
       "\n",
       "   800.mp4_ssim  900.mp4_psnr  900.mp4_qm  900.mp4_ssim  average_psnr  \\\n",
       "0      0.018558      0.352403    0.103225      0.025399     -0.626638   \n",
       "1      0.009657      0.798743    0.128736      0.020600      0.855944   \n",
       "\n",
       "   average_ssim  average_qm  score  \n",
       "0      0.011379   -0.012920    0.8  \n",
       "1      0.004731    0.084644    0.9  \n",
       "\n",
       "[2 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('resources/results2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the plots of the metrics for the DSSIM model in the format I mentioned earlier:\n",
    "\n",
    "![plots](resources/Adam_DSSIM_144_10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the PNSR model:\n",
    "\n",
    "![plots](resources/Adam_PNSR_144_10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the model trained on the PNSR loss outperformed the cubic interpolation on average and on 9 out of 10 so we decided to give it even more training, once again uptraining with 108 videos and 3 epochs, the results are presented in the table below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100.mp4_psnr</th>\n",
       "      <th>100.mp4_qm</th>\n",
       "      <th>100.mp4_ssim</th>\n",
       "      <th>1000.mp4_psnr</th>\n",
       "      <th>1000.mp4_qm</th>\n",
       "      <th>1000.mp4_ssim</th>\n",
       "      <th>200.mp4_psnr</th>\n",
       "      <th>200.mp4_qm</th>\n",
       "      <th>200.mp4_ssim</th>\n",
       "      <th>...</th>\n",
       "      <th>800.mp4_psnr</th>\n",
       "      <th>800.mp4_qm</th>\n",
       "      <th>800.mp4_ssim</th>\n",
       "      <th>900.mp4_psnr</th>\n",
       "      <th>900.mp4_qm</th>\n",
       "      <th>900.mp4_ssim</th>\n",
       "      <th>average_psnr</th>\n",
       "      <th>average_ssim</th>\n",
       "      <th>average_qm</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_360_720_252videos_AdamWopt_PNSRloss_1fb_...</td>\n",
       "      <td>0.511012</td>\n",
       "      <td>0.038215</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>1.020389</td>\n",
       "      <td>0.106209</td>\n",
       "      <td>0.00593</td>\n",
       "      <td>0.931552</td>\n",
       "      <td>0.134291</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796978</td>\n",
       "      <td>0.111497</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.782417</td>\n",
       "      <td>0.128722</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>0.959765</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.105079</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  100.mp4_psnr  \\\n",
       "0  small_360_720_252videos_AdamWopt_PNSRloss_1fb_...      0.511012   \n",
       "\n",
       "   100.mp4_qm  100.mp4_ssim  1000.mp4_psnr  1000.mp4_qm  1000.mp4_ssim  \\\n",
       "0    0.038215      0.000199       1.020389     0.106209        0.00593   \n",
       "\n",
       "   200.mp4_psnr  200.mp4_qm  200.mp4_ssim  ...  800.mp4_psnr  800.mp4_qm  \\\n",
       "0      0.931552    0.134291      0.018415  ...      0.796978    0.111497   \n",
       "\n",
       "   800.mp4_ssim  900.mp4_psnr  900.mp4_qm  900.mp4_ssim  average_psnr  \\\n",
       "0      0.014835      0.782417    0.128722      0.021093      0.959765   \n",
       "\n",
       "   average_ssim  average_qm  score  \n",
       "0      0.008715    0.105079    1.0  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('resources/results3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally our model came on top on all 10 videos and increased the average QM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the plots for the final training session:\n",
    "\n",
    "![plots](resources/Adam_PNSR_252_10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Environment\n",
    "\n",
    "- Ubuntu 24.04.1, Linux 6.8.0\n",
    "- Intel Core i5-13500\n",
    "- 32GB RAM\n",
    "- NVIDIA GeForce RTX 3060\n",
    "- NVIDIA Driver 535.183.01\n",
    "- CUDA 12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Dong, C., Loy, C.C., He, K. and Tang, X., 2015. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), pp.295-307.\n",
    "- Kappeler, A., Yoo, S., Dai, Q. and Katsaggelos, A.K., 2016. Video super-resolution with convolutional neural networks. IEEE transactions on computational imaging, 2(2), pp.109-122.\n",
    "- https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c\n",
    "- https://www.v7labs.com/blog/image-super-resolution-guide\n",
    "- https://alexandrosstergiou.github.io/datasets/Inter4K/index.html (Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points\n",
    "\n",
    "| Item                                         | Points |\n",
    "| -------------------------------------------- | ------ |\n",
    "| Super-Resolution                             | 3      |\n",
    "| Own Architecture                             | 2      |\n",
    "| Non-trivial solution (using mutliple frames) | 1      |\n",
    "| At least 10,000 images                       | 1      |\n",
    "| Our own dataset                              | 1      |\n",
    "| Testing 3 optimizers                         | 1      |\n",
    "| Testing 3 loss functions                     | 1      |\n",
    "| MlFLow                                       | 1      |\n",
    "| Streamlit GUI                                | 1      |\n",
    "| DVC                                          | 2      |\n",
    "| Total                                        | 14     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Github Repository](https://github.com/Dawid64/super-enhanced-resolution)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
